{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c390181-0046-440d-80f5-88bf511a438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "####################################### FOR TESTING ####################################################\n",
    "########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12b5deb-7104-41cb-a453-203f7db1f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffb0f39b-a05b-41a5-bb11-37166b122add",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x7bd8ded126d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from procrustes import smart_procrustes_align_gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotnine import ggplot, aes, geom_col, theme_classic, scale_fill_manual, labs, element_text, theme\n",
    "from spacy.lang.en import English\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "nlp = English(pipeline=[])\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1c6f59d-61d5-42ca-a55b-98cbf018ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(text: str) -> list[list[str]]:\n",
    "    \"\"\"Split the specified text into sentences, consisting of text tokens.\"\"\"\n",
    "\n",
    "    sents = []\n",
    "\n",
    "    # We process the text in chunks by paragraph, ensuring that a sentence\n",
    "    # never crosses a paragraph boundary:\n",
    "    for para in text.split(\"\\n\\n\"):\n",
    "        doc = nlp(para.replace(\"\\n\", \" \"))\n",
    "        for sent in doc.sents:\n",
    "            tokens = [\n",
    "                token.text.lower().strip() for token in sent if not token.is_space\n",
    "            ]\n",
    "            sents.append(tokens)\n",
    "\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee771ba4-70fa-4014-b2e8-2debfa63a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_print(vec, target: str, subName: str, n: int = 10):\n",
    "\n",
    "    print(\"Top \" + str(n) + \" words similar to \" + target + \" in \" + subName + \":\")\n",
    "\n",
    "    for word, value in vec.most_similar(target, topn=n):\n",
    "\n",
    "        print(f\"{value: .2f} {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0428ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus_path : str, corpora_path=\"/corpora\"):\n",
    "    \"\"\"\n",
    "    Return a tokenized version of the corpus at corpus_path\n",
    "    \"\"\"\n",
    "\n",
    "    # Read and tokenize corpus from disc\n",
    "    with open(os.path.join(corpora_path, corpus_path)) as f:\n",
    "        raw = f.read()\n",
    "        tokenized = get_sentences(raw)\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8133c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aligned_embedding_spaces(corpus1: str, corpus2: str):\n",
    "    \"\"\"\n",
    "    Create an embedding space for corpus1 and corpus 2,\n",
    "    align corpus 2 to corpus 1, and return their embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: ignore words with low frequencies\n",
    "    # TODO: find ideal threshold\n",
    "\n",
    "    model1 = Word2Vec(corpus1, epochs=25, window=4)\n",
    "    model2 = Word2Vec(corpus2, epochs=25, window=4)\n",
    "\n",
    "    print(\"Word2Vec models generated\")\n",
    "\n",
    "    model2_aligned = smart_procrustes_align_gensim(model1, model2)\n",
    "\n",
    "    print(\"Embedding spaces aligned\")\n",
    "\n",
    "    embeddings1 = model1.wv\n",
    "    embeddings2 = model2_aligned.wv\n",
    "\n",
    "    return (embeddings1, embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd799c5-d327-4722-83e1-c6387e976dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and tokenize dem corpus from disc\n",
    "with open(\"corpora/demText.txt\") as f:\n",
    "    demText = f.read()\n",
    "    demSents = get_sentences(demText)\n",
    "\n",
    "# Read and tokenize rep corpus from disc\n",
    "with open(\"corpora/repText.txt\") as f:\n",
    "    repText = f.read()\n",
    "    repSents = get_sentences(repText)\n",
    "\n",
    "# TODO: ignore words with low frequencies\n",
    "# TODO: find ideal threshold\n",
    "\n",
    "demModel = Word2Vec(demSents, epochs=25, window=4)\n",
    "repModel = Word2Vec(repSents, epochs=25, window=4)\n",
    "\n",
    "print(\"Word2Vec models generated\")\n",
    "\n",
    "repModelAligned = smart_procrustes_align_gensim(demModel, repModel)\n",
    "\n",
    "print(\"Embedding spaces aligned\")\n",
    "\n",
    "demVec = demModel.wv\n",
    "repVec = repModelAligned.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a25790bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_least_similar(embeddings1, embeddings2, topn=10) -> dict:\n",
    "    \"\"\"\n",
    "    Return the topn words with the lowest cosine similarity\n",
    "    across the two embedding spaces and their similarity score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get list of tokens used in both embedding spaces (intersection of both vocabularies)\n",
    "    vocab = list(set(embeddings1.index_to_key) & set(embeddings2.index_to_key)) \n",
    "\n",
    "    # Filter to only tokens that are in both datasets\n",
    "    vec1 = {token: embeddings1[token] for token in vocab}\n",
    "    vec2 = {token: embeddings2[token] for token in vocab}\n",
    "\n",
    "    # Calculate each token's cosine similarity across the two embedding spaces\n",
    "    similarities = {token: cosine(vec1[token], vec2[token]) for token in vocab}\n",
    "\n",
    "    # Lower index --> less similar\n",
    "    least_sim = sorted(similarities, key=lambda x: similarities[x], reverse=False)\n",
    "\n",
    "    # Get the vectors of the 10 least similar words across embedding spaces\n",
    "    bottom_n_words = least_sim[:1000]\n",
    "    bottom_n_sims = {token: similarities[token] for token in bottom_n_words}\n",
    "\n",
    "    return bottom_n_sims\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414dce1d-61af-49ac-bade-da5d3d6c7175",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_least_similar(demVec, repVec)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2d596a-69d9-49d3-81cc-f6c62dccfeb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1634414882.py, line 18)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"corpi\" : set(corpus1, corpus2) # Since order is not supposed to matter\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "corpora_filepath = \"/corpora\"\n",
    "corpora = os.listdir(corpora_filepath)\n",
    "corpus_comparisons = []\n",
    "\n",
    "# Maybe a way to do all of this with matrix operations?\n",
    "for corpus1 in corpora:\n",
    "    for corpus2 in corpora:\n",
    "        # No point in comparing a corpora's similarity to itself\n",
    "        if corpus1 == corpus2:\n",
    "            pass\n",
    "        else:\n",
    "            tokens1 = tokenize(corpus1)\n",
    "            tokens2 = tokenize(corpus2)\n",
    "            embeddings1, embeddings2 = get_aligned_embedding_spaces(tokens1, tokens2)\n",
    "            least_similar = get_least_similar(embeddings1, embeddings2, topn=100)\n",
    "            corpus_comparisons.append({\n",
    "                \"corpi\" : set(corpus1, corpus2), # Since order is not supposed to matter\n",
    "                \"similarities\" : least_similar\n",
    "            })\n",
    "\n",
    "            print(f\"CORPUS 1: {corpus1}\")\n",
    "            print(f\"CORPUS 2: {corpus2}\")\n",
    "            for token in least_similar:\n",
    "                print(f\"TOKEN: {token}    SIMILARITY: {least_similar[token]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353dba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"taxes\"\n",
    "words = []\n",
    "values = []\n",
    "communities = []\n",
    "\n",
    "for word, value in demVec.most_similar(TARGET, topn=10):\n",
    "    words.append(word)\n",
    "    values.append(value)\n",
    "    communities.append(\"r/democrats\")\n",
    "\n",
    "for word, value in repVec.most_similar(TARGET, topn=10):\n",
    "    words.append(word)\n",
    "    values.append(value)\n",
    "    communities.append(\"r/republicans\")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Community\" : communities,\n",
    "    \"Word\" : words,\n",
    "    \"Similarity\" : values\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd154ea-2b4b-471a-bf61-9b490db3d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (ggplot(df, aes(x=\"Word\", y=\"Similarity\", fill=\"Community\")) +\n",
    " geom_col(position=\"dodge\") +\n",
    " scale_fill_manual(values={\"r/republicans\" : \"red\", \"r/democrats\" : \"blue\"}) +\n",
    " labs(title=f'Semantic Similarity between \"{TARGET}\" and Related Words') +\n",
    " theme(axis_text_x=element_text(angle=45)))\n",
    "\n",
    "p.show()\n",
    "\n",
    "p.save(\"taxes.png\")\n",
    "\n",
    "similarity_print(demVec, TARGET, \"democrats\")\n",
    "similarity_print(repVec, TARGET, \"republicans\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
